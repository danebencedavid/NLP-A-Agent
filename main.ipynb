{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPaHbsXz56X9eeYqjzINOEW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danebencedavid/NLP-A-Agent/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from nltk.corpus import wordnet\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import requests\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ECTkjNrzhsra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading the metadata.csv for CORD-19**."
      ],
      "metadata": {
        "id": "Dvsw5e1wilO-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w_jIVnqhZuL"
      },
      "outputs": [],
      "source": [
        "\n",
        "path = kagglehub.dataset_download(\"googleai/dataset-metadata-for-cord19\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(path)"
      ],
      "metadata": {
        "id": "01Yo_wL2hbVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename_with_path = path + \"/\" + os.listdir(path)[0]\n",
        "filename_with_path"
      ],
      "metadata": {
        "id": "XP3tiCoXhrye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_meta_cord19 = pd.read_csv(filename_with_path)"
      ],
      "metadata": {
        "id": "U_VCnZ1ph00R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_meta_cord19 = df_meta_cord19[df_meta_cord19['description'].notnull()]"
      ],
      "metadata": {
        "id": "r67R4EJ1h5KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting keywords from  *description* column.**"
      ],
      "metadata": {
        "id": "tXuNImL9ioBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "vX9WPQ91iwzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spacy_keywords(text):\n",
        "    keywords = []\n",
        "    for text in tqdm(text, desc=\"Extracting keywords\", unit=\"abstracts\"):\n",
        "        if pd.isna(text):\n",
        "            keywords.append([])\n",
        "        else:\n",
        "            doc = nlp(text)\n",
        "            keywords.append([chunk.text.lower() for chunk in doc.noun_chunks][:10])\n",
        "    return keywords"
      ],
      "metadata": {
        "id": "lzOZTf0xkB8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_meta_cord19['keywords'] = spacy_keywords(df_meta_cord19['description'])"
      ],
      "metadata": {
        "id": "hPWEz1Q1kH9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_meta_cord19['keywords'][0]"
      ],
      "metadata": {
        "id": "EpqvHsg_l5rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Keyword Preprocessing**"
      ],
      "metadata": {
        "id": "LcdbMd0Jv-cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_keyword(keyword):\n",
        "    keyword = re.sub(r'[^\\w\\s]', '', keyword.lower())\n",
        "    keyword = ' '.join([lemmatizer.lemmatize(word) for word in keyword.split()])\n",
        "    return keyword.strip()"
      ],
      "metadata": {
        "id": "16uOFzAasTz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_meta_cord19['processed_keywords'] = df_meta_cord19['keywords'].apply(lambda lst: list(set([preprocess_keyword(k) for k in lst])))"
      ],
      "metadata": {
        "id": "JP6r72wXv7oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_meta_cord19['processed_keywords'][0]"
      ],
      "metadata": {
        "id": "fXgPhuuowhL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Keyword expansion**"
      ],
      "metadata": {
        "id": "YHLF15ezxHKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smoking_terms = {\"smoking\", \"tobacco\", \"cigarette\"}\n",
        "covid_terms = {\"covid\", \"sars-cov-2\", \"coronavirus\"}\n",
        "socio_terms = {\"poverty\", \"income\", \"education\", \"social class\", \"disadventage\",\"inequality\"}\n",
        "\n",
        "def get_synonyms(seed_term):\n",
        "    synonyms = set()\n",
        "    try:\n",
        "        for syn in wordnet.synsets(seed_term):\n",
        "            for lemma in syn.lemmas():\n",
        "                term = lemma.name().lower().replace('_', ' ')\n",
        "                if len(term.split()) <= 2:\n",
        "                    synonyms.add(term)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching synonyms for {seed_term}: {e}\")\n",
        "    return synonyms"
      ],
      "metadata": {
        "id": "qWbwQRkqwifx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_wikidata(keyword):\n",
        "    terms = set()\n",
        "    try:\n",
        "        url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={keyword}&language=en&format=json\"\n",
        "        response = requests.get(url, timeout=10).json()\n",
        "        terms.update(item['label'].lower() for item in response.get('search', []))\n",
        "    except Exception as e:\n",
        "        print(f\"Wikidata API error: {e}\")\n",
        "    return terms"
      ],
      "metadata": {
        "id": "yh7mlP7wxSpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smoking_terms.update(get_synonyms(\"smoking\"))\n",
        "smoking_terms.update(query_wikidata(\"tobacco\"))\n",
        "covid_terms.update(get_synonyms(\"covid\"))\n",
        "covid_terms.update(query_wikidata(\"SARS-CoV-2\"))\n",
        "socio_terms.update(query_wikidata(\"poverty\"))\n",
        "socio_terms.update(query_wikidata(\"income\"))\n",
        "socio_terms.update(query_wikidata(\"education\"))\n",
        "socio_terms.update(query_wikidata(\"social class\"))\n",
        "socio_terms.update(query_wikidata(\"disadvantage\"))\n",
        "socio_terms.update(query_wikidata(\"inequality\"))"
      ],
      "metadata": {
        "id": "NFmF_lslxYPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filtering papers**"
      ],
      "metadata": {
        "id": "16cvy7J3z2b-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "8xeg-8Tyz7XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smoking_embeddings = model.encode(list(smoking_terms))\n",
        "covid_embeddings = model.encode(list(covid_terms))\n",
        "third_topic_embeddings = model.encode(list(socio_terms))"
      ],
      "metadata": {
        "id": "qqiilHGp0DwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_match(text, term_embeddings, threshold=0.75):\n",
        "    if pd.isna(text):\n",
        "        return False\n",
        "    text_embed = model.encode(text)\n",
        "    similarities = np.dot(text_embed, term_embeddings.T)\n",
        "    return np.max(similarities) >= threshold"
      ],
      "metadata": {
        "id": "zTO2OdoH0MJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_mask = df_meta_cord19['processed_keywords'].apply(\n",
        "    lambda x: (\n",
        "        semantic_match(x, smoking_embeddings) and\n",
        "        semantic_match(x, covid_embeddings) and\n",
        "        semantic_match(x, socio_terms)\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "3GGWeEIl0WqA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}